{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from random import sample, seed\n",
    "seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#-------------------------- set gpu using tf ---------------------------\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "#config.gpu_options.per_process_gpu_memory_fraction = 0.3\n",
    "session = tf.Session(config=config)\n",
    "#-------------------  start importing keras module ---------------------\n",
    "from keras.layers import Input, Convolution2D, MaxPooling2D, Activation, concatenate, Dropout, GlobalAveragePooling2D, Flatten, Dense\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.datasets import cifar10\n",
    "from keras.callbacks import TensorBoard\n",
    "# from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "from keras import models\n",
    "from keras import optimizers\n",
    "from keras import applications\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from scipy.misc import imresize\n",
    "import tensorflow as tf\n",
    "from skimage.io import imread\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras.callbacks as callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./fer2013/fer2013.csv', dtype={'emotion':np.int32, 'pixels':str, 'Usage':str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['pixels'] = df['pixels'].apply(lambda x: np.fromstring(x,sep=' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df.loc[df['Usage'] == 'Training']\n",
    "validation = df.loc[df['Usage'] == 'PublicTest']\n",
    "test = df.loc[df['Usage'] == 'PrivateTest']\n",
    "\n",
    "y_train = pd.get_dummies(train['emotion'])\n",
    "y_train.columns = ['Angry','Disgust','Fear','Happy','Sad','Surprise','Neutral']\n",
    "\n",
    "y_val = pd.get_dummies(validation['emotion'])\n",
    "y_val.columns = ['Angry','Disgust','Fear','Happy','Sad','Surprise','Neutral']\n",
    "\n",
    "y_test = pd.get_dummies(test['emotion'])\n",
    "y_test.columns = ['Angry','Disgust','Fear','Happy','Sad','Surprise','Neutral']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.vstack(train['pixels'].values)\n",
    "x_validation = np.vstack(validation['pixels'].values)\n",
    "x_test = np.vstack(test['pixels'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.stack((np.reshape(x_train,(-1, 48, 48, 1)),)*3, axis=-2).squeeze()\n",
    "X_val = np.stack((np.reshape(x_validation,(-1, 48, 48, 1)),)*3, axis=-2).squeeze()\n",
    "X_test =  np.stack((np.reshape(x_test,(-1, 48, 48, 1)),)*3, axis=-2).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_color(color_files, y, folder):\n",
    "    numbers = []\n",
    "    for item in color_files:\n",
    "        for subitem in item.split('_'):\n",
    "            if(subitem.isdigit()):\n",
    "                numbers.append(subitem)\n",
    "\n",
    "    X_color = np.empty((len(numbers), 48, 48, 3)).astype('uint8')\n",
    "    y_color = np.empty((len(numbers), 7))\n",
    "\n",
    "    for i in range(len(numbers)):\n",
    "\n",
    "        path = folder + color_files[i]\n",
    "        val = int(numbers[i])\n",
    "\n",
    "        X_color[i] = np.array(imread(path)).astype('uint8')\n",
    "        y_color[i] = y.iloc[val]\n",
    "        \n",
    "    return X_color, y_color, numbers\n",
    "\n",
    "    #np.save('train_data', colorVal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_color_files = [k for k in os.listdir('train_color/') if '_color' in k]\n",
    "\n",
    "# X_train, y_train, indices_train = get_color(train_color_files, y_train, './train_color/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_testval, y_train, y_testval = train_test_split(X_train, y_train, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_val, X_test, y_val, y_test = train_test_split(X_testval, y_testval, test_size=0.5, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/cpu:0'):\n",
    "    tf_x_train = tf.placeholder(tf.float32, shape=(len(X_train), 48, 48, 3))\n",
    "    tf_x_validation = tf.placeholder(tf.float32, shape=(len(X_val), 48, 48, 3))\n",
    "    tf_x_test = tf.placeholder(tf.float32, shape=(len(X_test), 48, 48, 3))\n",
    "\n",
    "    tf_x_train_resized = tf.image.resize_images(tf_x_train,  size=[75,75])\n",
    "    tf_x_validation_resized = tf.image.resize_images(tf_x_validation,  size=[75,75])\n",
    "    tf_x_test_resized = tf.image.resize_images(tf_x_test,  size=[75,75])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n",
    "    X_train,X_val,X_test = sess.run([tf_x_train_resized,tf_x_validation_resized,tf_x_test_resized], feed_dict={tf_x_train: X_train,\n",
    "                                                   tf_x_validation: X_val,\n",
    "                                                   tf_x_test: X_test\n",
    "                                                  })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28709, 75, 75, 3)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "        rotation_range=40,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        rescale=1./255,\n",
    "        fill_mode='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#datagen = ImageDataGenerator(\n",
    "#    samplewise_center=True,\n",
    "#    samplewise_std_normalization=True,\n",
    "#    horizontal_flip=True,\n",
    "#    vertical_flip=False)\n",
    "\n",
    "# compute quantities required for featurewise normalization\n",
    "# (std, mean, and principal components if ZCA whitening is applied)\n",
    "datagen.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_imagenet = applications.vgg16.VGG16(include_top=False, weights='imagenet', input_shape=(75, 75, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 75, 75, 3)         0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 75, 75, 64)        1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 75, 75, 64)        36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 37, 37, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 37, 37, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 37, 37, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 18, 18, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 18, 18, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 18, 18, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 18, 18, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 9, 9, 256)         0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 9, 9, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 2, 2, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_imagenet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'block5_pool'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_imagenet.layers[-1].name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add new classification layers\n",
    "x = model_imagenet.layers[-1].output\n",
    "x = Flatten()(x)\n",
    "x = Dense(7)(x)\n",
    "x = Activation('softmax', name='new_loss')(x)\n",
    "\n",
    "#new Model\n",
    "model = Model(model_imagenet.inputs, x, name='model_new')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 75, 75, 3)         0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 75, 75, 64)        1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 75, 75, 64)        36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 37, 37, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 37, 37, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 37, 37, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 18, 18, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 18, 18, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 18, 18, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 18, 18, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 9, 9, 256)         0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 9, 9, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 7)                 14343     \n",
      "_________________________________________________________________\n",
      "new_loss (Activation)        (None, 7)                 0         \n",
      "=================================================================\n",
      "Total params: 14,729,031\n",
      "Trainable params: 14,729,031\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_1 False\n",
      "block1_conv1 True\n",
      "block1_conv2 True\n",
      "block1_pool True\n",
      "block2_conv1 True\n",
      "block2_conv2 True\n",
      "block2_pool True\n",
      "block3_conv1 True\n",
      "block3_conv2 True\n",
      "block3_conv3 True\n",
      "block3_pool True\n",
      "block4_conv1 True\n",
      "block4_conv2 True\n",
      "block4_conv3 True\n",
      "block4_pool True\n",
      "block5_conv1 True\n",
      "block5_conv2 True\n",
      "block5_conv3 True\n",
      "block5_pool True\n",
      "flatten_1 True\n",
      "dense_1 True\n",
      "new_loss True\n"
     ]
    }
   ],
   "source": [
    "#for layer in model.layers:\n",
    "#    layer.trainable = True\n",
    "\n",
    "for layer in model.layers:\n",
    "    print(layer.name, layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 2, 2, 512)\n",
      "(?, 7)\n"
     ]
    }
   ],
   "source": [
    "print(model_imagenet.output.shape)\n",
    "print(model.output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we compile our model and train it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_val = 2**7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tbCallBack = callbacks.TensorBoard(log_dir = \"./tensorboard/\")\n",
    "early_stopping_callback = callbacks.EarlyStopping(monitor='val_acc', patience=5, mode='max')\n",
    "checkpoint_callback = callbacks.ModelCheckpoint('vgg16_fine_tuning.h5', monitor='val_acc', verbose=1, save_best_only=True, mode='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "225/224 [==============================] - 224s 995ms/step - loss: 1.5913 - acc: 0.3706 - val_loss: 1.3599 - val_acc: 0.5021\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.50209, saving model to vgg16_fine_tuning.h5\n",
      "Epoch 2/100\n",
      "225/224 [==============================] - 228s 1s/step - loss: 1.2897 - acc: 0.5064 - val_loss: 1.2315 - val_acc: 0.5302\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.50209 to 0.53023, saving model to vgg16_fine_tuning.h5\n",
      "Epoch 3/100\n",
      "225/224 [==============================] - 237s 1s/step - loss: 1.1819 - acc: 0.5532 - val_loss: 1.1488 - val_acc: 0.5648\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.53023 to 0.56478, saving model to vgg16_fine_tuning.h5\n",
      "Epoch 4/100\n",
      "225/224 [==============================] - 235s 1s/step - loss: 1.1059 - acc: 0.5819 - val_loss: 1.0988 - val_acc: 0.5846\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.56478 to 0.58456, saving model to vgg16_fine_tuning.h5\n",
      "Epoch 5/100\n",
      "225/224 [==============================] - 235s 1s/step - loss: 1.0562 - acc: 0.6022 - val_loss: 1.1016 - val_acc: 0.5938\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.58456 to 0.59376, saving model to vgg16_fine_tuning.h5\n",
      "Epoch 6/100\n",
      "225/224 [==============================] - 236s 1s/step - loss: 1.0276 - acc: 0.6130 - val_loss: 1.0508 - val_acc: 0.6069\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.59376 to 0.60685, saving model to vgg16_fine_tuning.h5\n",
      "Epoch 7/100\n",
      "225/224 [==============================] - 235s 1s/step - loss: 0.9955 - acc: 0.6274 - val_loss: 1.0746 - val_acc: 0.5988\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.60685\n",
      "Epoch 8/100\n",
      "225/224 [==============================] - 235s 1s/step - loss: 0.9697 - acc: 0.6349 - val_loss: 1.0640 - val_acc: 0.6004\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.60685\n",
      "Epoch 9/100\n",
      "225/224 [==============================] - 237s 1s/step - loss: 0.9448 - acc: 0.6466 - val_loss: 1.0147 - val_acc: 0.6116\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.60685 to 0.61159, saving model to vgg16_fine_tuning.h5\n",
      "Epoch 10/100\n",
      "225/224 [==============================] - 236s 1s/step - loss: 0.9171 - acc: 0.6529 - val_loss: 1.0026 - val_acc: 0.6205\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.61159 to 0.62051, saving model to vgg16_fine_tuning.h5\n",
      "Epoch 11/100\n",
      "225/224 [==============================] - 236s 1s/step - loss: 0.8996 - acc: 0.6626 - val_loss: 1.0079 - val_acc: 0.6233\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.62051 to 0.62329, saving model to vgg16_fine_tuning.h5\n",
      "Epoch 12/100\n",
      "225/224 [==============================] - 237s 1s/step - loss: 0.8715 - acc: 0.6746 - val_loss: 0.9924 - val_acc: 0.6328\n",
      "\n",
      "Epoch 00012: val_acc improved from 0.62329 to 0.63277, saving model to vgg16_fine_tuning.h5\n",
      "Epoch 13/100\n",
      "225/224 [==============================] - 235s 1s/step - loss: 0.8579 - acc: 0.6814 - val_loss: 0.9854 - val_acc: 0.6247\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.63277\n",
      "Epoch 14/100\n",
      "225/224 [==============================] - 236s 1s/step - loss: 0.8395 - acc: 0.6878 - val_loss: 0.9915 - val_acc: 0.6294\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.63277\n",
      "Epoch 15/100\n",
      "225/224 [==============================] - 236s 1s/step - loss: 0.8206 - acc: 0.6948 - val_loss: 0.9648 - val_acc: 0.6392\n",
      "\n",
      "Epoch 00015: val_acc improved from 0.63277 to 0.63918, saving model to vgg16_fine_tuning.h5\n",
      "Epoch 16/100\n",
      "225/224 [==============================] - 234s 1s/step - loss: 0.8096 - acc: 0.6975 - val_loss: 0.9720 - val_acc: 0.6406\n",
      "\n",
      "Epoch 00016: val_acc improved from 0.63918 to 0.64057, saving model to vgg16_fine_tuning.h5\n",
      "Epoch 17/100\n",
      "225/224 [==============================] - 235s 1s/step - loss: 0.7855 - acc: 0.7083 - val_loss: 0.9881 - val_acc: 0.6350\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.64057\n",
      "Epoch 18/100\n",
      "225/224 [==============================] - 237s 1s/step - loss: 0.7717 - acc: 0.7092 - val_loss: 0.9959 - val_acc: 0.6425\n",
      "\n",
      "Epoch 00018: val_acc improved from 0.64057 to 0.64252, saving model to vgg16_fine_tuning.h5\n",
      "Epoch 19/100\n",
      "225/224 [==============================] - 235s 1s/step - loss: 0.7533 - acc: 0.7177 - val_loss: 0.9926 - val_acc: 0.6478\n",
      "\n",
      "Epoch 00019: val_acc improved from 0.64252 to 0.64781, saving model to vgg16_fine_tuning.h5\n",
      "Epoch 20/100\n",
      "225/224 [==============================] - 235s 1s/step - loss: 0.7308 - acc: 0.7276 - val_loss: 0.9978 - val_acc: 0.6342\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.64781\n",
      "Epoch 21/100\n",
      "225/224 [==============================] - 236s 1s/step - loss: 0.7176 - acc: 0.7352 - val_loss: 0.9674 - val_acc: 0.6414\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.64781\n",
      "Epoch 22/100\n",
      "225/224 [==============================] - 235s 1s/step - loss: 0.6979 - acc: 0.7419 - val_loss: 0.9601 - val_acc: 0.6598\n",
      "\n",
      "Epoch 00022: val_acc improved from 0.64781 to 0.65979, saving model to vgg16_fine_tuning.h5\n",
      "Epoch 23/100\n",
      "225/224 [==============================] - 235s 1s/step - loss: 0.6762 - acc: 0.7506 - val_loss: 0.9999 - val_acc: 0.6606\n",
      "\n",
      "Epoch 00023: val_acc improved from 0.65979 to 0.66063, saving model to vgg16_fine_tuning.h5\n",
      "Epoch 24/100\n",
      "225/224 [==============================] - 236s 1s/step - loss: 0.6643 - acc: 0.7578 - val_loss: 1.0113 - val_acc: 0.6523\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.66063\n",
      "Epoch 25/100\n",
      "225/224 [==============================] - 235s 1s/step - loss: 0.6465 - acc: 0.7625 - val_loss: 1.0235 - val_acc: 0.6486\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.66063\n",
      "Epoch 26/100\n",
      "225/224 [==============================] - 237s 1s/step - loss: 0.6297 - acc: 0.7655 - val_loss: 1.0090 - val_acc: 0.6567\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.66063\n",
      "Epoch 27/100\n",
      "225/224 [==============================] - 237s 1s/step - loss: 0.6099 - acc: 0.7732 - val_loss: 1.0360 - val_acc: 0.6456\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.66063\n",
      "Epoch 28/100\n",
      "225/224 [==============================] - 235s 1s/step - loss: 0.5972 - acc: 0.7810 - val_loss: 1.0450 - val_acc: 0.6520\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.66063\n"
     ]
    }
   ],
   "source": [
    "# Compile model and train it.\n",
    "\n",
    "model.compile(loss = \"categorical_crossentropy\", optimizer = optimizers.Adam(lr = 0.0001), metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit_generator(datagen.flow(X_train, y_train, batch_size=batch_size_val),\n",
    "                              validation_data=datagen.flow(X_val, y_val, batch_size=batch_size_val), \n",
    "                              steps_per_epoch=len(X_train) / batch_size_val, epochs=100, \n",
    "                              validation_steps=len(X_val)/batch_size_val,\n",
    "                              callbacks=[early_stopping_callback, checkpoint_callback, tbCallBack])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's evaluate on our test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss', 'acc']\n",
      "[1.0056260871036897, 0.6558930064250779]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on validation:\n",
    "# ...\n",
    "print(model.metrics_names)\n",
    "print(model.evaluate_generator(datagen.flow(X_val, y_val, batch_size=batch_size_val), steps=len(X_val)/batch_size_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss', 'acc']\n",
      "[0.9687015553243514, 0.6600724436108134]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test:\n",
    "# ...\n",
    "print(model.metrics_names)\n",
    "print(model.evaluate_generator(datagen.flow(X_test, y_test, batch_size=batch_size_val), steps=len(X_test)/batch_size_val))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
